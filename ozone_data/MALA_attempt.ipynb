{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting MALA with PyTorch's autograd\n",
    "\n",
    "#### Model: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{Latent:} \\quad X_t & = & A X_{t-1}  + \\nu_t, \n",
    "\\\\\n",
    "\\text{Observed:} \\quad Y_t & = & C X_t + B Z_t + \\omega_t, \\quad Z_t ~ \\text{are covariates}\n",
    "\\\\\n",
    "\\nu_t & \\sim & \\text{N}(0, Q ),\n",
    "\\\\\n",
    "\\omega_t & \\sim & \\text{N}(0, R).\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "from pykalman import KalmanFilter\n",
    "import numpy as np, numpy.random as npr, matplotlib.pyplot as plt, copy, multiprocessing as mp, torch, pandas\n",
    "from scipy.stats import *\n",
    "from pylab import plot, show, legend\n",
    "from tqdm import trange\n",
    "from ozone_functions import *\n",
    "from torch.distributions import multivariate_normal\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv(\"data.csv\").values\n",
    "data = data[:,1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = np.shape(data)[0]\n",
    "Y = torch.FloatTensor(data[:,0:3])\n",
    "Z = torch.FloatTensor(data[:,3::])\n",
    "\n",
    "obs_dim = np.shape(Y)[-1]\n",
    "lat_dim = 1\n",
    "cov_dim = np.shape(Z)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = torch.zeros(lat_dim,lat_dim, requires_grad=True)\n",
    "C = torch.randn(obs_dim,lat_dim, requires_grad=True)\n",
    "log_sigmay2 = torch.tensor(0., requires_grad=True)\n",
    "\n",
    "Q = torch.eye(lat_dim)\n",
    "R = torch.exp(log_sigmay2)*torch.eye(obs_dim)\n",
    "\n",
    "B = torch.randn(obs_dim,cov_dim, requires_grad=True)\n",
    "b = (torch.matmul(B,Z.transpose(0,1))).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu0 = torch.zeros(lat_dim)\n",
    "Sigma0 = torch.eye(lat_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filtering_mean = np.zeros((T,lat_dim))\n",
    "predictive_mean = np.zeros((T,lat_dim))\n",
    "filtering_cov = np.zeros((T,lat_dim,lat_dim))\n",
    "predictive_cov = np.zeros((T,lat_dim,lat_dim))\n",
    "\n",
    "filtering_mean[0] = mu0\n",
    "filtering_cov[0] = Sigma0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in trange(1000) :\n",
    "    for t in range(T) :\n",
    "        predictive_mean[t] = A.dot(filtering_mean[t])\n",
    "        predictive_cov[t] = A.dot(filtering_cov[t]).dot(A.transpose())\n",
    "        K = predictive_cov[t].dot(C.transpose()).dot(np.linalg.inv(C.dot(predictive_cov[t]).dot(C.transpose())+R))\n",
    "        filtering_mean[t] = predictive_mean[t] + K.dot(Y[t]-C.dot(predictive_mean[t]))\n",
    "        filtering_cov[t] = (np.eye(lat_dim)-K.dot(C)).dot(predictive_cov[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, torch.Size([3, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_dim, obs_dim, np.shape(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lpdf(Y, Z, A, C, B, log_sigmay2, mu0, Sigma0) :\n",
    "    T = np.shape(Y)[0]\n",
    "    filtering_mean = torch.clone(mu0.detach())\n",
    "    filtering_cov = torch.clone(Sigma0.detach())\n",
    "    \n",
    "    lat_dim = np.shape(A)[0]\n",
    "    obs_din = np.shape(C)[0]\n",
    "\n",
    "    lpdf = torch.tensor(0.)\n",
    "    Q = torch.eye(lat_dim)\n",
    "    R = torch.exp(log_sigmay2)*torch.eye(obs_dim)\n",
    "\n",
    "    for t in range(T) :\n",
    "        predictive_mean = torch.matmul(A,filtering_mean)\n",
    "        predictive_cov = torch.matmul(A,torch.matmul(filtering_cov,A.transpose(0,1)))\n",
    "\n",
    "        K = torch.matmul(torch.matmul(predictive_cov,C.transpose(0,1)),\\\n",
    "                         (torch.inverse(torch.matmul(torch.matmul(C,predictive_cov),C.transpose(0,1))+R)))\n",
    "        filtering_mean = predictive_mean + torch.matmul(K,(Y[t]- torch.matmul(C,predictive_mean)))\n",
    "        filtering_cov = torch.matmul(torch.eye(lat_dim) - torch.matmul(K,C), predictive_cov) + Q\n",
    "\n",
    "        mean = torch.matmul(C,filtering_mean) + b[t] \n",
    "        cov = torch.matmul(C,torch.matmul(filtering_cov,C.transpose(0,1))) + R\n",
    "\n",
    "        dist = multivariate_normal.MultivariateNormal(loc=mean,covariance_matrix=cov)\n",
    "        lpdf += dist.log_prob(Y[t])\n",
    "    \n",
    "    lpdf.backward(retain_graph=True)\n",
    "    \n",
    "    return lpdf, A.grad, C.grad, B.grad, log_sigmay2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07656693458557129\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "lpdf, A_grad, C_grad, B_grad, log_sigmay2_grad = get_lpdf(Y[:100], Z[:100], A, C, B, log_sigmay2, mu0, Sigma0)\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.]], dtype=float32), tensor([[-219742.5938],\n",
       "         [ 172460.3906],\n",
       "         [  79737.4141]]), tensor([[-243049.7656, -472102.0000],\n",
       "         [ 182992.8750,  377115.3750],\n",
       "         [  85311.6641,  173620.4844]]), tensor(333977.8125))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_grad.numpy(), C_grad, B_grad, log_sigmay2_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaptive_MALA(Y, Z, A, C, B, log_sigmay2, mu0, Sigma0, \n",
    "                  n_mcmc, tauA, tauC, tausy, adapt=True, start_adapt=0.2, power=1, kappa=1) :\n",
    "    \n",
    "    npr.seed()\n",
    "    scipy.random.seed()\n",
    "    \n",
    "    log_sigmay2_chain = torch.zeros(n_mcmc+1, requires_grad=False)\n",
    "    log_sigmay2_chain[0] = log_sigmay2\n",
    "    A_chain = torch.zeros((n_mcmc+1, *np.shape(A)), requires_grad=False)\n",
    "    C_chain = torch.zeros((n_mcmc+1, *np.shape(C)), requires_grad=False)\n",
    "    B_chain = torch.zeros((n_mcmc+1, *np.shape(B)), requires_grad=False)\n",
    "    A_chain[0], C_chain[0], B_chain[0] = A, C, B\n",
    "    \n",
    "    accepted = 0\n",
    "    last_accepted = 0\n",
    "    \n",
    "    start = time()\n",
    "    A_current = torch.clone(A.detach())\n",
    "    C_current = torch.clone(C.detach())\n",
    "    B_current = torch.clone(B.detach())\n",
    "    log_sigmay2_current = torch.clone(log_sigmay2.detach()) \n",
    "    \n",
    "    A_current.requires_grad = True\n",
    "    C_current.requires_grad = True\n",
    "    B_current.requires_grad = True\n",
    "    log_sigmay2_current.requires_grad = True\n",
    "    \n",
    "    for n in trange(n_mcmc) :\n",
    "        \n",
    "        ll_current, A_grad, C_grad, B_grad, log_sigmay2_grad \\\n",
    "        = get_lpdf(Y, Z, A_current, C_current, B_current, log_sigmay2_current, mu0, Sigma0)\n",
    "    \n",
    "        log_sigmay2_proposed = torch.tensor((log_sigmay2_current + tausy*log_sigmay2_grad \\\n",
    "                                             + torch.sqrt(2*tausy)*torch.randn(1)).detach().numpy(),\n",
    "                                            requires_grad=True)\n",
    "        A_proposed = torch.tensor((A_current + tauA*A_grad + torch.sqrt(2*tauA)*torch.randn(*np.shape(A))).detach().numpy(), \n",
    "                                  requires_grad=True)\n",
    "        C_proposed = torch.tensor((C_current + tauC*C_grad + torch.sqrt(2*tauC)*torch.randn(*np.shape(C))).detach().numpy(), \n",
    "                                  requires_grad=True)\n",
    "        B_proposed = torch.clone(B)\n",
    "        \n",
    "        if np.abs(A_proposed.detach().numpy()) < 1 :\n",
    "        \n",
    "            ll_proposed, A_grad_proposed, C_grad_proposed, B_grad_proposed, log_sigmay2_grad_proposed \\\n",
    "            = get_lpdf(Y, Z, A_proposed, C_proposed, B_proposed,\n",
    "                       log_sigmay2_proposed, mu0, Sigma0)\n",
    "\n",
    "            log_accept_ratio = power*(ll_proposed - ll_current)\n",
    "            bottom = -power/(4*tauA)*torch.sum(A_proposed-A_current-tauA*A_grad).detach()**2 \\\n",
    "                     -power/(4*tauC)*torch.sum(C_proposed-C_current-tauC*C_grad).detach()**2 \\\n",
    "                     -power/(4*tausy)*torch.sum(log_sigmay2_proposed-log_sigmay2_current-tausy*log_sigmay2_grad).detach()**2\n",
    "            top = -power/(4*tauA)*torch.sum(A_current-A_proposed-tauA*A_grad_proposed).detach()**2 \\\n",
    "                  -power/(4*tauC)*torch.sum(C_current-C_proposed-tauC*C_grad_proposed).detach()**2 \\\n",
    "                  -power/(4*tausy)*torch.sum(log_sigmay2_current-log_sigmay2_proposed-\\\n",
    "                                              tausy*log_sigmay2_grad_proposed).detach()**2\n",
    "\n",
    "            log_accept_ratio = log_accept_ratio + top-bottom\n",
    "        \n",
    "            if np.log(npr.rand()) < log_accept_ratio.detach().numpy() :\n",
    "                log_sigmay2_chain[n+1] = log_sigmay2_proposed.detach()\n",
    "                A_chain[n+1] = A_proposed.detach()\n",
    "                C_chain[n+1] = C_proposed.detach()\n",
    "                B_chain[n+1] = B_proposed.detach()\n",
    "                A_grad = A_grad_proposed.detach()\n",
    "                C_grad = C_grad_proposed.detach()\n",
    "                log_sigmay2_grad = log_sigmay2_grad_proposed.detach()\n",
    "                accepted += 1\n",
    "                last_accepted = n\n",
    "        else :\n",
    "            log_sigmay2_chain[n+1] = log_sigmay2_current.detach()\n",
    "            A_chain[n+1] = A_current.detach()\n",
    "            C_chain[n+1] = C_current.detach()\n",
    "            B_chain[n+1] = B_current.detach()\n",
    "\n",
    "    \n",
    "    print(100*accepted/n_mcmc, \"% acceptance rate\")\n",
    "    return log_sigmay2_chain.detach().numpy(), A_chain.detach().numpy(), C_chain.detach().numpy(), \\\n",
    "            B_chain.detach().numpy(), accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:01<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 % acceptance rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_mcmc = 500\n",
    "tauA = torch.tensor(1e-8)\n",
    "tauC = torch.tensor(1e-8)\n",
    "tausy = torch.tensor(1e-8)\n",
    "\n",
    "log_sigmay2_chain, A_chain, C_chain, B_chain, accepted = \\\n",
    "adaptive_MALA(Y[:100], Z[:100], A, C, B, log_sigmay2, mu0, Sigma0, n_mcmc, tauA, tauC, tausy, adapt=False, power=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0000000e+00,  5.0922812e-05, -3.1224961e-05,  2.1016075e-04,\n",
       "        8.2552287e-06,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_chain[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
